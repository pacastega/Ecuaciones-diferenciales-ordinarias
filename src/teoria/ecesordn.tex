\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{definition}
\label{def:sisescordsup}
	Una ecuación diferencial escalar de orden superior es una ecuación de la
	forma:
	\[x^{(n)} = a_n(t) x^{(n - 1)} + \cdots + a_1(t)x + a_0(t).\]
\end{definition}

Definiendo las variables \(x_n = x^{(n - 1)}\), o lo que es lo mismo,
\(x_n = x'_{n - 1}\), así el sistema~\ref{def:sisescordsup} es equivalente al
sistema:
\[\eqsys{
	x_2 &= x'_1 \\
	x_3 &= x'_2 \\
	&\vdots \\
	x_n &= x'_{n - 1} \\
	x'_n &= a_n(t) x_n + \cdots + a_1(t) x_1 + a_0(t)
	}\]

donde la última ecuación es de primer orden, podemos también expresar este
sistema vectorialmente como \(x' = A(t)x + b(t)\) con:
\[A(t) = \mat{ 
	0 & 1 & & \\
	& \ddots & \ddots & \\
	& & 0 & 1 \\
	a_1(t) & a_2(t) & \cdots & a_n(t)
	}
	\quad \text{y} \quad
	b(t) = \mat{0 \\ \vdots \\ 0 \\ a_0(t)}.
	\]

En este caso el problema del valor inicial consiste en fijar \(x_1(t_0), 
x_2(t_0), \dots, x_n(t_0)\). Aplicando lo visto anteriormente el sistema tiene
solución general dada por \(x_p(t) + x_n(t)\) donde, como de costumbre, 
\(x_p(t)\) es una solución particular y \(x_n(t)\) es la solución general de la
ecuación homogénea asociada, la cual es un espacio vectorial de dimensión \(n\).
Si \(\phi_1, \dots, \phi_n\) son soluciones linealmente independientes 
de~\ref{def:sisescordsup} la matriz fundamental asociada al sistema equivalente
es
\[\Phi(t) = \mat{
	\phi_1 & \dots & \phi_n \\ 
	\phi'_1 & \dots & \phi'_n \\ 
	\vdots & & \vdots \\ 
	\phi^{(n - 1)}_1 & \dots & \phi^{(n - 1)}_n \\ 
	}.\]

\begin{definition}
	Llamaremos Wronskiano de \(\phi_1, \dots, \phi_n\) al determinante
	\[W(\phi_1, \dots, \phi_n)(t) = \det\mat{
		\phi_1(t) & \dots & \phi_n(t) \\ 
		\phi'_1(t) & \dots & \phi'_n(t) \\ 
		\vdots & & \vdots \\ 
		\phi^{(n - 1)}_1(t) & \dots & \phi^{(n - 1)}_n(t) \\ 
	}, \quad t \in (\alpha, \omega).\]
\end{definition}

\section{Matriz fundamental}

Hemos dejado pendiente cómo obtener una matriz fundamental \(\Phi(t)\) asociada
al sistema homogéneo \(x' = A(t)x\), por ahora nos restringiremos al caso de
coeficientes constantes, es decir, \(x' = Ax\) donde \(A\) es una matriz
constante, no depende de \(t\) y nos centraremos en particular en funciones de 
dominio \(\R\).  
En el caso escalar, \(x' = ax\), sabemos que la solución es 
\(x = e^{at}\), en el caso matricial veremos que una matriz fundamental es
\(\Phi(t) = e^{At}\), aunque primero tendremos que definir la exponencial de una
matriz y obtener métodos para computarla.

\begin{lemma}
\label{lem:expphi}
	Sea \(\Phi(t)\) la matriz fundamental asociada a las soluciones de
	\[\eqsys{
		x' = Ax \\
		x(0) = e_j
		}\]
	o, equivalentemente, \(\Phi(t)\) es la única solución del problema de valor
	inicial
	\[\eqsys{
		X' = AX \\
		X(0) = \mathit{Id}
		}\]
	Se dan las siguientes igualdades:
	\begin{multicols}{3}
	\begin{enumerate}[i)]
		\item \(\displaystyle \Phi(0) = \mathit{Id}\) 
		\item \(\displaystyle \Phi(t + s) = \Phi(t) \Phi(s)\) 
		\item \(\displaystyle \Phi(-t) = \Phi^{-1}(t)\) 
	\end{enumerate}
	\end{multicols}
\end{lemma}

\begin{proof}
	\begin{enumerate}[i), wide, labelwidth=0pt, labelindent=0pt]
		\item La primera igualdad se sigue directamente de la definición.
		\item Fijamos \(s\) y consideramos el problema de valor inicial
			\[\eqsys{ x' = Ax \\ x(0) = \Phi(s) }\]
			Vamos a ver que tanto \(\Phi(t + s)\) como \(\Phi(t)\Phi(s)\) (como
			funciones en \(t\)) son soluciones de ese problema. Consideramos
			primero \(X(t) := \Phi(t + s)\), tenemos que \(X(0) = \Phi(s)\) y
			\[X'(t) = \Phi'(t + s) = A \Phi(t + s) = A X(t),\]
			similarmente para \(Y(t) := \Phi(t)\Phi(s)\), aplicando la primera 
			igualdad \(Y(0) = \Phi(0)\Phi(s) = \Phi(s)\) y
			\[Y'(t) = \Phi'(t)\Phi(s) = A \Phi(t)\Phi(s) = A Y(t).\]
		\item Aplicando la primera y segunda igualdad en los respectivos
			miembros:
			\[\mathit{Id} = \Phi(t + (-t)) = \Phi(t)\Phi(-t).\]
	\end{enumerate}
\end{proof}

Para la matriz fundamental \(\Phi\) así definida \(x(t) = \Phi(t)x^0\)
es solución del sistema
\[\eqsys{x' = Ax \\ x(0) = x^0}\]

\section{Exponencial de una matriz}

Notamos que la matriz que acabamos de definir \(\Phi\) cumple las
mismas propiedades que la función exponencial, de hecho más adelante
demostraremos que \(\Phi(t) = \sum_{k=0}^\infty (tA)^k/k!\), por ahora nos
contentamos con la siguiente definición:

\begin{definition}
	Dada una matriz \(B\) se define \(\exp(B)\), o con la notación habitual
	\(e^B\), como
	\[e^B = \sum_{k = 0}^\infty \frac{B^k}{k!}.\]
\end{definition}

Presentamos ahora algunas propiedades típicas de la función exponencial que
también cumple esta versión matricial, la mayoría son corolario de lo visto 
arriba.

\begin{proposition}
	Se cumplen las siguientes igualdades.
	\begin{multicols}{2}
	\begin{enumerate}[i)]
		\item \(\displaystyle e^0 = \mathit{Id}\) 
		\item \(\displaystyle e^{(t + s)A} = e^{tA} e^{sA}\) 
		\item \(\displaystyle \parens{e^{tA}}^{-1} = e^{-tA}\) 
		\item \(\displaystyle \frac{d}{\dif t} e^{tA} = A e^{tA}\)
	\end{enumerate}
	\end{multicols}
\end{proposition}

\begin{remark}
	No es cierto en general que \(e^{A + B} = e^A e^B\), solo se cumple si las
	matrices \(A\) y \(B\) conmutan.
\end{remark}

\begin{theorem}
	La matriz \(\Phi(t)\) definida en~\ref{lem:expphi} es igual a \(e^{At}\),
	dicho de otro modo:
	\[\Phi(t) = \sum_{k = 0}^\infty \frac{t^k A^k}{k!} = 
		\sum_{k = 0}^\infty \frac{(tA)^k}{k!}.\]
\end{theorem}

\begin{proof}
	Haremos uso de las iteraciones de Picard, para ello definimos
	\begin{align*}
		T : C((\alpha, \omega), M_{n \times n}) &\to 
			C((\alpha, \omega), M_{n \times n}) \\
		\Phi(t) &\mapsto \mathit{Id} + \int_0^t A \Phi(s) \dif s.
	\end{align*}

	Comenzamos la iteración con \(x_0(t) = \mathit{Id}\) y proseguimos:
	\begin{align*}
		x_1 &= T(x_0) = \mathit{Id} + \int_0^t A \mathit{Id} \dif s = 
			\mathit{Id} + At \\
		x_2 &= T(x_1) = \mathit{Id} + \int_0^t A (\mathit{Id} + As) \dif s = 
			\mathit{Id} + At + \frac{A^2 t^2}{2} \\
		&\vdots \\
		x_n &= T(x_{n - 1}) =
			\mathit{Id} + At + \frac{A^2 t^2}{2} + \cdots + \frac{A^n t^n}{n!}
			= \sum_{k = 0}^n \frac{A^k t^k}{k!},
	\end{align*}
	tomando el límite \(\lim_{n \to \infty} x_n\) obtenemos el resultado.
\end{proof}

\section{Métodos de cómputo}

\subsection{Matrices diagonales}

Podemos pasar ahora a ver cómo computar \(e^{At}\) para una matriz \(A\).
Comenzamos presentando un método para matrices diagonalizables. Sea \(A\) una
matriz diagonal, entonces:
\[A = \mat{
	\lambda_1 & & \\
	& \ddots & \\
	& & \lambda_n
	}
	\quad \text{y} \quad
	x' = Ax \iff
	\eqsys{
		x'_1 &= \lambda_1 x_1 \\
		&\vdots \\
		x'_n &= \lambda_n x_n \\
		}
	\]
La solución general de este sistema viene dada por 
\(x_1(t) = k_1 e^{\lambda_1 t}, \dots, x_n(t) = k_n e^{\lambda_n t}\) o lo que es
lo mismo:
\[x = \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}} 
	\mat{k_1 \\ \vdots \\ k_n}.\]

Puesto que es solución del problema de valor inicial 
\(\set{X' = AX \wedge X(0) = \mathit{Id}}\) hemos comprobado a posteriori, por 
la unicidad de la solución, que la matriz de la izquierda es \(e^{At}\). Para
convencernos lo comprobamos también directamente, tenemos que
\[e^{\smat{\lambda_1 & & \\ & \ddots & \\ & & \lambda_n} t}
	= \sum_{k = 0}^\infty 
	\frac{\smat{\lambda_1 & & \\ & \ddots & \\ & & \lambda_n}^k t^k}{k!} 
	= \sum_{k = 0}^\infty 
	\frac{\smat{\lambda_1 t & & \\ & \ddots & \\ & & \lambda_n t}^k}{k!},
	\]
ahora, explotando la diagonalidad
\[\sum_{k = 0}^\infty 
	\frac{\smat{\lambda_1 t & & \\ & \ddots & \\ & & \lambda_n t}^k}{k!}
	= \sum_{k = 0}^\infty 
	\frac{\smat{(\lambda_1 t)^k & & \\ & \ddots & \\ & & (\lambda_n t)^k}}{k!}
	= 
	\mat{\sum_{k = 0}^\infty \frac{(\lambda_1 t)^k}{k!} & & \\
		& \ddots & 
		\\ & & \sum_{k = 0}^\infty \frac{(\lambda_n t)^k}{k!}}.
	\]
Hemos demostrado así de dos formas distintas el siguiente teorema:

\begin{proposition}
	Sea \(A\) una matriz diagonal, podemos expresar \(e^{At}\) como:
	\[e^{At} = e^{\smat{\lambda_1 & & \\ & \ddots & \\ & & \lambda_n} t}
		= \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}}.\]
\end{proposition}

\subsection{Matrices diagonalizables}

Sea \(A\) una matriz diagonalizable con autovalores reales, sabemos, por álgebra
lineal, que existe una matriz \(P\) tal que si \(\lambda_1, \dots, \lambda_n\)
son los autovalores de \(A\) y \(v_1, \dots, v_n\) son sus autovectores;
\[
	P^{-1} A P = D = \mat{\lambda_1 && \\ & \ddots & \\ && \lambda_n}
	\quad \text{y} \quad
	P = (v_1, \dots, v_n).
\]

Con esto tenemos que la ecuación \(x' = Ax\) se puede escribir 
\(x' = P D P^{-1} x\), con el cambio de variable \(y = P^{-1}x\) nos queda:
\[(x' = Ax \iff x' = PDP^{-1}x) \iff (P^{-1}x' = DP^{-1}x \iff y' = y),\]
con lo que podemos expresar la ecuación como
\[y' = \mat{\lambda_1 && \\ & \dots & \\ && \lambda_n} y \iff
	y = e^{Dt}k \iff y = \mat{k_1 e^{\lambda_1 t} \\ \vdots \\
		k_n e^{\lambda_n t}},\]
con \(k = (k_1, \dots, k_n)^t \in \R^n\). Tras estos preámbulos podemos obtener 
una expresión explícita del espacio de soluciones
\begin{align*}
	x &= P y = (v_1, \dots, v_n) \mat{k_1 e^{\lambda_1 t} \\ \vdots \\
		k_n e^{\lambda_n t}} = k_1 e^{\lambda_1 t} v_1 + \cdots 
		+ k_n e^{\lambda_n t}v_n \\
		&= P \mat{e^{\lambda_1 t} && \\ & \dots & \\ && e^{\lambda_n t}}
		\mat{k_1 \\ \vdots \\ k_n} = P e^{D t} P^{-1} \mat{c_1 \\ \vdots 
			\\ c_n} = e^{A t} \mat{c_1 \\ \vdots \\ c_n}
\end{align*}

Notamos que \(e^{\lambda_1 t} v_1, \dots, e^{\lambda_n t} v_n\) es base del
espacio de soluciones. Con esto estamos preparados para afrontar el problema del
valor inicial.

\subsection{Problema del valor inicial}

Consideramos el problema del valor inicial dado por el sistema con matriz \(A\)
diagonalizable
\[\eqsys{
	x' = Ax \\
	x(0) = x^0
	}
	\iff 
	\eqsys{
		y' = Dy \\
		y(0) = P^{-1}x^0
	}\]
considerando el mismo cambio de referencia de antes. La solución única de este
sistema viene dada por \(y(t) = e^{Dt} P^{-1}x^0\), volviendo a la base original 
\(x(t) = e^{At} x^0\), donde hemos utilizado la unicidad de la solución. Igual
que en el caso diagonal se puede ver directamente que si \(A = PDP^{-1}\)
entonces \(e^{At} = P e^{Dt} P^{-1}\), aquí lo hemos visto indirectamente,
utilizando la unicidad de la solución. 

\end{document}
