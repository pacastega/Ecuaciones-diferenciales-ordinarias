\documentclass[../main.tex]{subfiles}

\begin{document}

Sean \(x_1, x_2, \dots, x_n \in C^1(\alpha, \omega)\) variables, las cuales son
funciones de clase \(C^1\) sobre un intervalo de \(\R\) y \(a_{ij}\), \(b_i\)
funciones continuas en \((\alpha, \omega)\) para todo \(i, j \in \N\).

\begin{definition}
	Un sistema de ecuaciones diferenciales de primer orden es un sistema de la
	forma:
	\[\eqsys{
			x'_1 &= f_1(t, x_1, x_2, \dots, x_n) \\
			x'_2 &= f_2(t, x_1, x_2, \dots, x_n) \\
				 &\vdots \\
			x'_n &= f_n(t, x_1, x_2, \dots, x_n)}\]
\end{definition}

\begin{definition}
\label{def:siseq1ord}
	Un sistema de ecuaciones diferenciales de primer orden lineal es de la
	forma:
	\[\eqsys{
		x'_1 &= a_{11}(t)x_1 + a_{12}(t)x_2 + \dots + a_{1n}(t)x_n + b_1(t) \\
		x'_2 &= a_{21}(t)x_1 + a_{22}(t)x_2 + \dots + a_{2n}(t)x_n + b_2(t) \\
			 &\vdots \\
		x'_n &= a_{n1}(t)x_1 + a_{n2}(t)x_2 + \dots + a_{nn}(t)x_n + b_n(t)}\]
\end{definition}

Podemos expresar un sistema de este tipo en forma matricial como
\[x' = Ax + b\]
donde
\[x = x(t) = \mat{x_1(t) \\ x_2(t) \\ \vdots \\ x_n(t)}, \quad
  A = A(t) = \mat{a_{ij}(t)}^n_{i,j = 1},\quad 
  b = b(t) = \mat{b_1(t) \\ b_2(t) \\ \vdots \\ b_n(t)} \]
y las derivadas e integrales se aplican ``coordenada a coordenada", v.g.
\[x' = \mat{x'_1(t) \\ x'_2(t) \\ \vdots \\ x'_n(t)}
	 = \mat{x'_1 \\ x'_2 \\ \vdots \\ x'_n}, \quad
  A' = \mat{a'_{ij}(t)}^n_{i,j = 1}, \quad
  \int A(s) \dif s = \mat{\int a_{ij} \dif s}^n_{i,j = 1}.\]

\section{Problema del valor inicial}

\begin{definition}
	Sean \(t_0 \in (\alpha, \omega)\) y 
	\(x^0 = \mat{x^0_1, x^0_2, \dots, x^0_n}^t \in \R^n\), el problema del valor
	inicial asociado a~\ref{def:siseq1ord} es de la forma
	\[\eqsys{
		x' = A(t)x + b(t) \\
		x(t_0) = x^0}\]
\end{definition}

Probaremos que el problema de valor inicial tiene solución única en todo el
dominio \((\alpha, \omega)\), para ello representaremos el problema del valor 
inicial como una ecuación integral, es una estrategia general en análisis 
intentar representar las funciones como integrales debido a su buen
comportamiento. Ser solución del problema del valor inicial equivale a
\[x(t) = x^0 + \int_{t_0}^t A(s)x(s) + b(s) \dif s.\]

Denotaremos por \(C(I, \R^n)\) el espacio vectorial de las funciones continuas
del intervalo \(I \subset \R\) en \(\R^n\).

A raíz del sistema~\ref{def:siseq1ord} definimos el operador integral
\begin{align*}
  T : C((\alpha, \omega), \R^n) &\to C((\alpha, \omega), \R^n) \\
  \phi  &\mapsto x^0 + \int_{t_0}^t A(s) \phi(s) + b(s) \dif s
\end{align*}

Evidentemente si \(x\) es solución del sistema se tiene \(T(x) = x\), por lo que
si demostramos que el operador \(T\) tiene un único punto fijo habremos
terminado. Hemos definido \(T\) con espacio de salida y llegada iguales para
poder aplicar el teorema del punto fijo de Banach, visto en cálculo diferencial,
para ello necesitaremos que el espacio sea también completo, recordamos la
definición de espacio métrico completo:

\begin{definition}
	Un espacio métrico es completo si toda sucesión de Cauchy es convergente.
\end{definition}

De esto se deduce que necesitaremos aplicar el teorema sobre un espacio
compacto, para ello sean
\(\alpha < \tilde{\alpha}\) y \(\tilde{\omega} < \omega\), probaremos que 
\(T\) tiene un único punto fijo como operador en 
\([\tilde{\alpha}, \tilde{\omega}]\) y haciendo tender 
\(\tilde{\alpha} \to \alpha\) y \(\tilde{\omega} \to \omega\) obtenemos el
resultado deseado. Recordamos también el enunciado del teorema del punto fijo:

\begin{theorem}[Punto fijo de Banach]
  Sean \((X, d)\) un espacio métrico completo y \(T : X \to X\) una aplicación
  contractiva (es decir, Lipschitz de constante \(\alpha \in [0, 1)\)), entonces
  \(T\) tiene un único punto fijo.
\end{theorem}

Este teorema tiene una consecuencia que nos será muy útil.

\begin{corollary}
	Sea \((X, d)\) un espacio métrico completo y \(T : X \to X\) tal que 
	\(T^n = T \circ \overset{n}{\cdots} \circ T\) es contractiva para algún
	\(n \in \N\), entonces \(T\) tiene un único punto fijo.
\end{corollary}

Definimos en \(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) la distancia 
\[d(f, g) = \norm{f - g}_{\infty} = 
	\max_{t \in [\tilde{\alpha}, \tilde{\omega}]} \norm{f(t) - g(t)}_2,\]
donde \(\norm{\cdot}_2\) es la norma euclídea en \(\R^n\). Así 
\(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) completo como queríamos.

\begin{lemma}
	El espacio \(C([\tilde{\alpha}, \tilde{\omega}], \R^n)\) con la distancia
	\(\norm{f - g}_\infty\) es un espacio métrico completo.
\end{lemma}

\begin{lemma}
\label{lem:desinfty}
	Para todo \(m \in \N\) se da la desigualdad:
	\[\norm{T^m(f)(t) - T^m(g)(t)}_2 \leq 
		\frac{\norm{A}^m_\infty}{m!} \abs{t - t_0}^m \norm{f - g}_\infty.\]
\end{lemma}

\begin{proof}
	El caso base \(m = 1\) está probado, suponemos el resultado cierto para 
	\(m\) entonces para \(m + 1\):
	\begin{align*}
		\norm{T^{m + 1}(f)(t) - T^{m + 1}(g)(t)}_2 
		&= \norm{T(T^m(f))(t) - T(T^m (g))(t)}_2 \\
		&\leq \int_{t_0}^t \norm{A(s)}_2 
			\norm{T^m(f)(t) - T^m (g)(t)}_2 \dif s,
	\end{align*}
	por la hipótesis de inducción:
	\begin{align*}
		\int_{t_0}^t \norm{A(s)}_2 
			\norm{T^m(f)(t) - T^m (g)(t)}_2 \dif s
		&\leq \int_{t_0}^t \norm{A}_\infty 
			\frac{\norm{A}^m_\infty}{m!} \abs{s - t_0}^m 
			\norm{f - g}_\infty \dif s \\
		&\leq \frac{\norm{A}^{m + 1}_\infty}{m!} \norm{f - g}_\infty 
			\int_{t_0}^t \abs{s - t_0}^m \dif s,
	\end{align*}
	ahora sin más que integrar:
	\[
		\frac{\norm{A}^{m + 1}_\infty}{m!} \norm{f - g}_\infty 
			\frac{\abs{t - t_0}^{m + 1}}{m + 1}
		= \frac{\norm{A}^{m + 1}_\infty}{(m + 1)!} \abs{t - t_0}^{m + 1} 
			\norm{f - g}_\infty,
	\]
	con lo que obtenemos el resultado.
\end{proof}

Esto implica que el operador \(T\) cumple la propiedad de Lipschitz.

\begin{corollary}
	Se da la desigualdad
	\[\norm{T^m(f) - T^m(g)}_\infty \leq 
		\frac{\norm{A}^m_\infty}{m!} (\tilde{\omega} - \tilde{\alpha})^m
		\norm{f - g}_\infty.\]
\end{corollary}

\begin{proof}
	Tomamos supremos en \(t\) en el lema~\ref{lem:desinfty}.
\end{proof}

Hemos demostrado así el teorema de Picard:

\begin{theorem}[Picard]
	Si \(A(t)\), \(b(t)\) son continuas en \((\alpha, \omega) \in \R\), 
	\(t_0 \in \R\) y \(x^0 \in \R^n\) el problema del valor inicial:
	\[\eqsys{
		x' = A(t)x + b(t) \\
		x(t_0) = x^0
		}\]
	tiene solución única en \((\alpha, \omega)\).
\end{theorem}

\begin{remark}
  En realidad no es necesaria la linealidad del operador: la misma demostración
  prueba la existencia y unicidad de la solución pidiendo únicamente
  la Lipschitzianidad del operador.
\end{remark}

Observamos que \(T^m(f_0)\), donde \(f_0\) es un punto inicial cualquiera,
converge exponencialmente rápido al punto fijo de \(T\). Esto nos da un método
numérico para aproximar la solución del PVI escogiendo un punto inicial simple
(constante). También nos proporciona el desarrollo en serie (analítico) de la
solución del PVI, este método se conoce como método de las iteraciones de
Picard.

\begin{example}
	Consideramos el PVI:
	\[\eqsys{x' = ax \quad \text{(ecuación escalar)} \\
		x(0) = x_0}\]

	Iniciamos el proceso con la solución constante \(x_0\), 
	\[x_1 = T(x_0) = x_0 + \int_0^t a x_0 \dif s = x_0 + x_0 a t,\]
	reiterando tenemos que
	\[x_n = T(x_{n - 1}) = x_0 \sum_{k = 0}^n \frac{(at)^k}{k!},\] cuando \(n\)
    tiende a infinito nos queda \(x_\infty := \lim_n x_n = x_0 e^{at}\).
\end{example}

\section{Estructura del espacio de soluciones}

\subsection{Caso homogéneo}

Consideramos el sistema homogéneo \(x' = A(t) x\) y definimos la función:
\begin{align*}
	T : C^1((\alpha, \omega), \R^n) &\to C \\
	x &\mapsto x' - A(t)x,
\end{align*}
claramente \(x\) es solución de la ecuación si y solo si \(x \in \ker(T)\).
Puesto que \(T\) es lineal el conjunto de soluciones es un espacio vectorial,
más propiamente un subespacio vectorial de \(C^1((\alpha, \omega), \R^n)\).

\begin{theorem}
	El espacio de soluciones de un sistema de ecuaciones lineales homogéneas es
	un subespacio vectorial de \(C^1((\alpha, \omega), \R^n)\) de dimensión 
	\(n\). Además si \(\phi_j\) es la solución del problema de valor inicial:
	\[\eqsys{
		x' = A(t)x \\
		x(t_0) = e_j = (0, \dots, \underset{j}{1}, \dots, 0)^T
		}\]
	donde \(t_0 \in (\alpha, \omega)\) arbitrario, tenemos que
	\(\set{\phi_1, \dots, \phi_n}\) es base del susodicho espacio de soluciones.
\end{theorem}

\begin{proof}
	Demostramos primero que los \(\phi_j\) son linealmente independientes,
	evaluando \(\sum_{j = 1}^n \alpha_j \phi_j\) en \(t_0\):
	\[\sum_{j = 1}^n \alpha_j e_j = 0 \implies \alpha_j = 0,\]
	para todo \(j = 1, \dots, n\) como queríamos. Demostramos ahora que es
	sistema de generadores. Sea \(y\) solución de la ecuación lineal y sea
	\(y(t_0) = (\alpha_1, \dots, \alpha_n)^T\), definimos 
	\(z = \sum_{j = 1}^n \alpha_j \phi_j\) y observamos que tanto \(y\) como
	\(z\) son soluciones de
	\[\eqsys{
		x' = A(t)x \\
		x(t_0) = (\alpha_1, \dots, \alpha_n)^T
		}\]
	por la unicidad de la solución concluimos que \(y = z\).
\end{proof}

\begin{definition}
	Llamaremos matriz fundamental del sistema \(x' = A(t)x\) y la denotaremos
	\(\Phi(t)\) a cualquier matriz cuyas columnas formen base del espacio de 
	soluciones del sistema.
\end{definition}

Con las definiciones anteriores tenemos que \(\Phi(t) := 
(\phi_1, \phi_2, \dots, \phi_n)\) es una matriz fundamental. Por la propia 
definición tenemos que si \(\Phi\) matriz fundamental de un sistema
de ecuaciones lineales homogéneas  entonces la solución general del sistema es
\(\Phi(t)c\) con \(c \in \R^n\):
\[x(t) = \Phi(t) c = (\phi_1, \dots, \phi_n) \mat{c_1 \\ \vdots \\ c_n} =
	c_1\phi_1 + \cdots + c_n\phi_n.\]

\begin{corollary}
	Sea \(\Phi(t)\) una matriz cuyas columnas son solución de la ecuación
	\(x' = A(t)x\), entonces \(\Phi(t)\) es matriz fundamental, o lo que es lo
	mismo:
	\[\det(\Phi(t)) \neq 0 \ \forall t \in (\alpha, \omega)
		\iff \exists t_0 \in (\alpha, \omega) \ \det(\Phi(t_0)) \neq 0\]
\end{corollary}

La afirmación: las columnas de \(\Phi(t)\) son soluciones de \(x' = A(t)x\) es
equivalente a: \(\Phi(t)\) es solución de la ecuación matricial \(X' = A(t)X\),
donde
\begin{align*}
	X : (\alpha, \omega) &\to M_{n \times n} \\
	t &\mapsto x(t)
\end{align*}

Asimismo \(\Phi(t)\) matriz fundamental equivale a que es solución de un
problema de valor inicial de la forma:
\[\eqsys{
	X' = A(t)X \\
	X(t_0) = X^0
}\]
donde ambas ecuaciones son matriciales y \(\det(X_0) \neq 0\). Notamos que la 
matriz fundamental \(\Phi(t) = (\phi_1, \dots, \phi_n)\) es la única solución
del problema de valor inicial:
\[\eqsys{
	X' = A(t)X \\
	X(0) = \mathit{Id}
	}\]

\subsection{Caso no homogéneo}

La misma demostración del caso escalar prueba que la solución general de 
\(x' = A(t)x + b(t)\) es \(x_p(t) + x_n(t)\) donde \(x_p(t)\) es una solución
particular arbitraria del sistema y \(x_n(t)\) es solución del sistema
homogéneo asociado: \(x' = A(t)x\), por lo visto arriba, \(x_n(t) = \Phi(t)c\)
con \(c \in \R^n\). Cómo veremos existen diversos métodos para obtener 
\(x_p(t)\).

\section{Método de variación de constantes}

Un método para obtener una solución particular \(x_p(t)\) de un sistema 
\(x' = A(t)x + b(t)\) es la variación de constantes, que como veremos es muy
similar al al caso de una variable. Sabemos que la solución general de la
ecuación homogénea asociada es \(x_n(t) = \Phi(t)c\), conjeturamos la existencia
de \(x_p(t) = \Phi(t) c(t)\):
\[x'_p(t) = \Phi'(t) c(t) + \Phi(t) c'(t) = A(t )\Phi(t) c(t) + \Phi(t) c'(t).\]

Además, puesto que es solución del sistema homogéneo
\[x'_p(t) = A(t)x_p(t) + b(t) = [A(t) \Phi(t) c(t)] + b(t),\]

igualando ambas expresiones
\[c'(t) = \Phi^{-1}(t) b(t).\]

Basta por tanto elegir \(c(t) = \int_{t_0}^t \Phi^{-1}(s) b(s) \dif s\), hemos
demostrado el siguiente teorema:

\begin{theorem}
	Sea \(\Phi(t)\) matriz fundamental del sistema \(x' = A(t)x\), entonces 
	\[x_p(t) = \Phi(t) \int_{t_0}^t \Phi^{-1}(s) b(s) \dif s\]
	con \(t_0 \in (\alpha, \omega)\) es una solución particular de 
	\(x' = A(t)x + b(t)\) que satisface \(x_p(t_0) = \vec{0}\).
\end{theorem}

\end{document}
