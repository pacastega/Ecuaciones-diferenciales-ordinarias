\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{definition}
\label{def:sisescordsup}
	Una ecuación diferencial escalar de orden superior es una ecuación de la
	forma:
	\[x^{(n)} = a_n(t) x^{(n - 1)} + \cdots + a_1(t)x + a_0(t).\]
\end{definition}

Definiendo las variables \(x_n = x^{(n - 1)}\), o lo que es lo mismo,
\(x_n = x'_{n - 1}\), el sistema~\ref{def:sisescordsup} es equivalente a:
\[\eqsys{
	x_2 &= x'_1 \\
	x_3 &= x'_2 \\
	&\vdots \\
	x_n &= x'_{n - 1} \\
	x'_n &= a_n(t) x_n + \cdots + a_1(t) x_1 + a_0(t)
	}\]

donde la última ecuación es de primer orden, podemos también expresar este
sistema matricialmente como \(x' = A(t)x + b(t)\) con:
\[A(t) = \mat{ 
	0 & 1 & & \\
	& \ddots & \ddots & \\
	& & 0 & 1 \\
	a_1(t) & a_2(t) & \cdots & a_n(t)
	}
	\quad \text{y} \quad
	b(t) = \mat{0 \\ \vdots \\ 0 \\ a_0(t)}.
	\]

En el caso del sistema el problema del valor inicial consiste en fijar \(x_1(t_0), 
x_2(t_0), \dots, x_n(t_0)\), lo que equivale aquí a fijar \(x(t_0), x'(t_0),
\dots, x^{(n-1)}(t_0)\). Aplicando lo visto anteriormente el sistema tiene
solución general dada por \(x_p(t) + x_h(t)\) donde, como de costumbre, 
\(x_p(t)\) es una solución particular y \(x_h(t)\) es la solución general de la
ecuación homogénea asociada, la cual es un espacio vectorial de dimensión \(n\).

Si \(\phi_1, \dots, \phi_n\) son soluciones linealmente independientes 
de~\ref{def:sisescordsup} la matriz fundamental asociada al sistema equivalente
es
\[\Phi(t) = \mat{
	\phi_1 & \dots & \phi_n \\ 
	\phi'_1 & \dots & \phi'_n \\ 
	\vdots & & \vdots \\ 
	\phi^{(n - 1)}_1 & \dots & \phi^{(n - 1)}_n \\ 
	}.\]

\begin{definition}
	Llamaremos Wronskiano de \(\phi_1, \dots, \phi_n\) al determinante
	\[W(\phi_1, \dots, \phi_n)(t) = \det\mat{
		\phi_1(t) & \dots & \phi_n(t) \\ 
		\phi'_1(t) & \dots & \phi'_n(t) \\ 
		\vdots & & \vdots \\ 
		\phi^{(n - 1)}_1(t) & \dots & \phi^{(n - 1)}_n(t) \\ 
	}, \quad t \in (\alpha, \omega).\]
\end{definition}

\section{Matriz fundamental}

Hemos dejado pendiente cómo obtener una matriz fundamental \(\Phi(t)\) asociada
al sistema homogéneo \(x' = A(t)x\), por ahora nos restringiremos al caso de
coeficientes constantes, es decir, \(x' = Ax\) donde \(A\) es una matriz
constante, no depende de \(t\) y nos centraremos en particular en funciones de 
dominio \(\R\).  

En el caso escalar, \(x' = ax\), sabemos que la solución es \(x = ke^{at}\),
donde podemos interpretar \(e^{at}\) como una ``matriz fundamental
\(1 \times 1\)''. En el caso matricial veremos que una matriz fundamental es
\(\Phi(t) = e^{At}\), aunque primero tendremos que definir la exponencial de una
matriz y obtener métodos para computarla.

\begin{lemma}
\label{lem:expphi}
	Sea \(\Phi(t)\) la matriz fundamental asociada a las soluciones de
	\[\eqsys{
		x' = Ax \\
		x(0) = e_j
		}\]
      o, equivalentemente, la única solución del problema de valor inicial
	\[\eqsys{
		X' = AX \\
		X(0) = \mathit{Id}
		}\]
	Se dan las siguientes igualdades:
	\begin{multicols}{3}
	\begin{enumerate}[i)]
		\item \(\displaystyle \Phi(0) = \mathit{Id}\) 
		\item \(\displaystyle \Phi(t + s) = \Phi(t) \Phi(s)\) 
		\item \(\displaystyle \Phi(-t) = \Phi^{-1}(t)\) 
	\end{enumerate}
	\end{multicols}
\end{lemma}

\begin{proof}
	\begin{enumerate}[i), wide, labelwidth=0pt, labelindent=0pt]
		\item La primera igualdad se sigue directamente de la definición.
		\item Fijamos \(s\) y consideramos el problema de valor inicial
			\[\eqsys{ x' = Ax \\ x(0) = \Phi(s) }\]
			Vamos a ver que tanto \(\Phi(t + s)\) como \(\Phi(t)\Phi(s)\) (como
			funciones en \(t\)) son soluciones de ese problema. Consideramos
			primero \(X(t) := \Phi(t + s)\), tenemos que \(X(0) = \Phi(s)\) y
			\[X'(t) = \Phi'(t + s) = A \Phi(t + s) = A X(t),\]
			similarmente para \(Y(t) := \Phi(t)\Phi(s)\), aplicando la primera 
			igualdad \(Y(0) = \Phi(0)\Phi(s) = \Phi(s)\) y
			\[Y'(t) = \Phi'(t)\Phi(s) = A \Phi(t)\Phi(s) = A Y(t).\]
		\item Aplicando la primera y segunda igualdad en los respectivos
			miembros:
			\[\mathit{Id} = \Phi(t + (-t)) = \Phi(t)\Phi(-t).\]
	\end{enumerate}
\end{proof}

Para la matriz fundamental \(\Phi\) así definida, \(x(t) = \Phi(t)x^0\)
es la solución del sistema
\[\eqsys{x' = Ax \\ x(0) = x^0}\]

\section{Exponencial de una matriz}

Notamos que la matriz que acabamos de definir \(\Phi\) tiene muchas de las
propiedades de la función exponencial, de hecho más adelante demostraremos que
\(\Phi(t) = \sum_{k=0}^\infty (tA)^k/k!\), por ahora nos contentamos con la
siguiente definición:

\begin{definition}
	Dada una matriz \(B\) se define \(\exp(B)\), o con la notación habitual
	\(e^B\), como
	\[e^B = \sum_{k = 0}^\infty \frac{B^k}{k!}.\]
\end{definition}

Presentamos ahora algunas propiedades típicas de la función exponencial que
también cumple esta versión matricial, la mayoría son corolario de lo visto 
arriba.

\begin{proposition}
	Se cumplen las siguientes igualdades.
	\begin{multicols}{2}
	\begin{enumerate}[i)]
		\item \(\displaystyle e^0 = \mathit{Id}\) 
		\item \(\displaystyle e^{(t + s)A} = e^{tA} e^{sA}\) 
		\item \(\displaystyle \parens{e^{tA}}^{-1} = e^{-tA}\) 
		\item \(\displaystyle \frac{d}{\dif t} e^{tA} = A e^{tA}\)
	\end{enumerate}
	\end{multicols}
\end{proposition}

\begin{remark}
	No es cierto en general que \(e^{A + B} = e^A e^B\), solo se cumple si las
	matrices \(A\) y \(B\) conmutan.
\end{remark}

\begin{theorem}
	La matriz \(\Phi(t)\) definida en~\ref{lem:expphi} es igual a \(e^{At}\),
	dicho de otro modo:
	\[\Phi(t) = \sum_{k = 0}^\infty \frac{t^k A^k}{k!} = 
		\sum_{k = 0}^\infty \frac{(tA)^k}{k!}.\]
\end{theorem}

\begin{proof}
	Haremos uso de las iteraciones de Picard, para ello definimos
	\begin{align*}
		T : C((\alpha, \omega), \mathcal{M}_{n \times n}) &\to 
			C((\alpha, \omega), \mathcal{M}_{n \times n}) \\
		\Psi &\mapsto \mathit{Id} + \int_0^t A \Psi(s) \dif s.
	\end{align*}

	Comenzamos la iteración con \(X_0(t) = \mathit{Id}\) y proseguimos:
	\begin{align*}
		X_1 &= T(X_0) = \mathit{Id} + \int_0^t A \mathit{Id} \dif s = 
			\mathit{Id} + At \\
		X_2 &= T(X_1) = \mathit{Id} + \int_0^t A (\mathit{Id} + As) \dif s = 
			\mathit{Id} + At + \frac{A^2 t^2}{2} \\
		&\vdots \\
		X_n &= T(X_{n - 1}) =
			\mathit{Id} + At + \frac{A^2 t^2}{2} + \cdots + \frac{A^n t^n}{n!}
			= \sum_{k = 0}^n \frac{A^k t^k}{k!},
	\end{align*}
	tomando el límite \(\lim_{n \to \infty} X_n\) obtenemos el resultado.
\end{proof}

\subsection{Métodos de cómputo}

Podemos pasar ahora a ver cómo computar \(e^{At}\) para una matriz \(A\).
Comenzamos presentando un método para matrices diagonalizables. Sea \(A\) una
matriz diagonal, entonces:
\[A = \mat{
	\lambda_1 & & \\
	& \ddots & \\
	& & \lambda_n
	}
	\quad \text{y} \quad
	x' = Ax \iff
	\eqsys{
		x'_1 &= \lambda_1 x_1 \\
		&\vdots \\
		x'_n &= \lambda_n x_n \\
		}
	\]
La solución general de este sistema viene dada por 
\(x_1(t) = k_1 e^{\lambda_1 t}, \dots, x_n(t) = k_n e^{\lambda_n t}\) o lo que es
lo mismo:
\[x = \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}} 
	\mat{k_1 \\ \vdots \\ k_n}.\]

Puesto que es solución del problema de valor inicial 
\(\set{X' = AX \wedge X(0) = \mathit{Id}}\) hemos comprobado a posteriori, por 
la unicidad de la solución, que la matriz de la izquierda es \(e^{At}\). Para
convencernos lo comprobamos también directamente, tenemos que
\[e^{\smat{\lambda_1 & & \\ & \ddots & \\ & & \lambda_n} t}
	= \sum_{k = 0}^\infty 
	\frac{\smat{\lambda_1 & & \\ & \ddots & \\ & & \lambda_n}^k t^k}{k!} 
	= \sum_{k = 0}^\infty 
	\frac{\smat{\lambda_1 t & & \\ & \ddots & \\ & & \lambda_n t}^k}{k!},
	\]
ahora, explotando la diagonalidad
\[\sum_{k = 0}^\infty 
	\frac{\smat{\lambda_1 t & & \\ & \ddots & \\ & & \lambda_n t}^k}{k!}
	= \sum_{k = 0}^\infty 
	\frac{\smat{(\lambda_1 t)^k & & \\ & \ddots & \\ & & (\lambda_n t)^k}}{k!}
	= 
	\mat{\sum_{k = 0}^\infty \frac{(\lambda_1 t)^k}{k!} & & \\
		& \ddots & 
		\\ & & \sum_{k = 0}^\infty \frac{(\lambda_n t)^k}{k!}}.
	\]
Hemos demostrado así de dos formas distintas el siguiente teorema:

\begin{proposition}
	Sea \(A\) una matriz diagonal, podemos expresar \(e^{At}\) como:
	\[e^{At} = \exp\smat{\lambda_1t & & \\ & \ddots & \\ & & \lambda_nt}
		= \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}}.\]
\end{proposition}

Sea \(A\) una matriz diagonalizable con autovalores reales; por definición, si
\(\lambda_1, \dots, \lambda_n\) son los autovalores de \(A\) y
\(v_1, \dots, v_n\) son autovectores asociados, entonces
\[
	P^{-1} A P = D = \mat{\lambda_1 && \\ & \ddots & \\ && \lambda_n}
	\quad \text{siendo} \quad
	P = (v_1, \dots, v_n).
\]

Con esto tenemos que la ecuación \(x' = Ax\) se puede escribir 
\(x' = P D P^{-1} x\), con el cambio de variable \(y = P^{-1}x\) nos queda:
\[x' = Ax \iff x' = PDP^{-1}x \iff P^{-1}x' = DP^{-1}x \iff y' = Dy,\]
donde es fundamental que \(P\) (y por tanto \(P^{-1}\)) sea una matriz constante
para poder afirmar \(y' = (P^{-1}x)' = P^{-1}x'\).

De esta forma, la solución de la ecuación transformada es

\[y = e^{Dt}k = \mat{e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t}}
    \mat{k_1 \\ \vdots \\ k_n} = \mat{k_1e^{\lambda_1 t} \\ \vdots \\
      k_ne^{\lambda_n t}} = \sum_{i=1}^n k_i e^{\lambda_i t} e_i,\]

  siendo \(k^t = (k_1, \dots, k_n) \in \R^n\) y \(\{e_1, \dots, e_n\}\) la base
  canónica de \(\R^n\). Por tanto,
  
  \[x = Py = (v_1, \dots, v_n) \mat{k_1e^{\lambda_1 t} \\ \vdots \\
      k_ne^{\lambda_n t}} = \sum_{i=1}^n k_i e^{\lambda_i t} v_i\]

  \begin{remark}
    Una base del espacio de soluciones de \(x' = Ax\) es \(\{e^{\lambda_1 t} v_1,
      \dots, e^{\lambda_n t} v_n\}\).
  \end{remark}

  Para el PVI

  \[
    \begin{cases}
      x' = Ax \\
      x(0) = x^0
    \end{cases} \iff
    \begin{cases}
      y' = Dy \\
      y(0) = P^{-1}x^0
    \end{cases}
  \]

  la solución única es \(y(t) = e^{Dt}P^{-1}x^0 \iff x(t) = Pe^{Dt}P^{-1}x^0 =
  e^{At}x^0\). Por la unicidad de la solución al PVI, esto equivale a \(e^{At} =
  Pe^{Dt}P^{-1}\).

  Consideramos ahora el caso diagonalizable general (\(A\) puede tener
  autovalores complejos).

  \begin{proposition}
    Si \(A\) tiene coeficientes reales y \(\lambda\) es autovalor de \(A\),
    entonces su conjugado \(\bar{\lambda}\) también lo es.
    \begin{proof}
      El polinomio característico de \(A\), \(p(\lambda) = \det(A-\lambda I)\)
      tiene coeficientes reales, es decir, \(p(\lambda) = \sum_{i=1}^n
      a_i\lambda^i\) con \(a_i \in \R\).

      Si \(\lambda_0 \in \Complex\) es raíz del polinomio, es decir, \(\sum_{i=1}^n
      a_i\lambda_0^i = 0\), entonces

      \[0 = \bar{0} = \overline{\sum_{i=1}^n a_i\lambda_0^i} = \sum_{i=1}^n
        \overline{a_i}(\overline{\lambda_0})^i = \sum_{i=1}^n
        a_i(\overline{\lambda_0})^i = p(\overline{\lambda_0})\]
    \end{proof}
  \end{proposition}

  \begin{remark}
    Esto quiere decir que los autovalores no reales siempre vienen por pares,
    por lo que nos restringiremos de momento a subespacios de dimensión 2.
  \end{remark}

  \begin{remark}
    En estas condiciones, ya sabemos resolver \(x' = Ax\) en \(\Complex\), es
    decir, si permitimos como solución \(z(t) = x(t) + iy(t)\) porque todo lo
    que hemos visto funciona igual para números complejos. Vamos a ver cómo
    aprovechar esto para transformar una solución con números complejos en otra
    equivalente que sólo involucre números reales.
  \end{remark}

  \begin{proposition}
    Si \(\lambda = a + ib\) es un autovalor complejo de \(A\) y \(w = u + iv\)
    un autovector asociado, entonces \(\bar{\lambda} = a - ib\) es también
    autovalor y \(\bar{w} = u - iv\) es un autovector asociado.
    \begin{proof}
      Como \(\lambda\) es un autovalor con autovector \(w\), se cumple
      \(Aw = \lambda w\), por lo que
      \(\overline{Aw} = \overline{\lambda w} = \bar{\lambda} \bar{w}\); por otro
      lado, \(\overline{Aw} = \bar{A} \bar{w} = A \bar{w}\), luego \(A \bar{w} =
      \bar{\lambda} \bar{w}\).
    \end{proof}
  \end{proposition}

  Sabemos, por lo visto anteriormente, que \(z(t) = e^{\lambda t} w\) y
  \(\bar{z}(t) = e^{\bar{\lambda} t} \bar{w}\) son soluciones de \(x' = Ax\) y
  son linealmente independientes. Sin más que desarrollar los productos, se
  llega a:
  \begin{gather*}
    z(t) = e^{(a+ib)t} (u+iv) = e^{at}\left( (u\cos bt - v\sin bt) + i(v\cos bt + u\sin bt) \right) \\
    \bar{z}(t) = e^{(a-ib)t} (u-iv) = e^{at}\left( (u\cos bt - v\sin bt) - i(v\cos bt + u\sin bt) \right)
  \end{gather*}

  Como
  \[\mat{\Re z(t) \\ \Im z(t)} = \frac{1}{2} \mat{z(t) + \bar{z}(t) \\
      z(t) - \bar{z}(t)} = \frac{1}{2}\mat{1 & 1 \\ 1 & -1}\mat{z(t) \\
      \bar{z}(t)} \]

  y \(\det\smat{1 & 1 \\ 1 & -1} \neq 0\), se cumple que \(\Re z(t)\) y
  \(\Im z(t)\) siguen siendo soluciones y linealmente independientes, que además
  generan el mismo espacio de soluciones. Tenemos entonces las soluciones reales
  \[
    \begin{cases}
      x(t) = \Re z(t) = e^{at}(u\cos bt - v\sin bt) \\
      y(t) = \Im z(t) = e^{at}(v\cos bt + u\sin bt)
    \end{cases}
  \]
  Consideramos \(A \in \mathcal{M}_{2 \times 2}(\R)\) con autovalores
  \(\lambda = a+ib, \bar{\lambda} = a-ib \in \Complex\), con autovectores
  asociados \(w = u+iv, \bar{w} = u-iv\). Se tiene
  \[Au + iAv = Aw = \lambda w = (a+ib)(u+iv) = (au-bv) + i(av+bu),\]
  de donde, igualando partes real e imaginaria,
  \[
  \begin{cases}
    Au = au-bv \\
    Av = bu+av
  \end{cases}
  \]
  Ya hemos visto que \(u\) y \(v\) son linealmente independientes, lo que nos
  permite definir una matriz de paso \(P = \mat{u & v}\). Entonces,
  \[\mat{Au & Av} = A \mat{u & v} = \mat{au-bv & bu+av} = \mat{u & v} \mat{a & b
      \\ -b & a} \implies P^{-1}AP = \mat{a & b \\ -b & a}\]
  Nos centramos ahora en sistemas de la forma \(x' = \smat{a & b \\ -b & a}x\),
  que tienen por solución general \(x(t) = \exp \smat{at & bt \\ -bt & at}
  \smat{c_1 \\ c_2}\), \(\smat{c_1 \\ c_2} \in \R^2\). Nos falta ahora calcular
  esta exponencial.
  \begin{align*}
    \exp\mat{at & bt \\ -bt & at} &= \exp\left(a\mat{1 & 0 \\ 0 & 1}t + b\mat{0 & 1 \\
    -1 & 0}t\right) = \mat{e^{at} & 0 \\ 0 & e^{at}} \exp\left(b\mat{0 & 1
    \\ -1 & 0}t\right) \\
    &= e^{at}\exp\left(b\mat{0 & 1 \\ -1 & 0}t\right),
  \end{align*}

  por lo que nos basta analizar el caso \(y' = \smat{0 & 1 \\ -1 & 0}y\), es
  decir, calcular \(\exp \smat{0 & t \\ -t & 0}\). Para ello, diagonalizamos la
  matriz \(M := \smat{0 & 1 \\ -1 & 0}\):

  Su polinomio característico es \(p(\lambda) = \lambda^2 + 1\), luego sus
  autovalores son \(\pm i\). Los espacios invariantes son, respectivamente,
  \[\ker(M - iI) = L\left[\mat{1 \\ i}\right] \quad \text{y} \quad \ker(M + iI) =
    L\left[\mat{i \\ 1}\right],\]

  con lo que una posible matriz de paso es \(P := \smat{1 & i \\ i & 1}\), con
  inversa \(P^{-1} = \frac{1}{2} \smat{1 & -i \\ -i & 1}\). Se tiene entonces
  \begin{gather*}
    \mat{0 & 1 \\ -1 & 0} = \mat{1 & i \\ i & 1} \mat{i & 0 \\ 0 & -i}
                             \mat{1 & i \\ i & 1} ^{-1} \implies \\
    \exp \mat{0 & t \\ -t & 0} = \frac{1}{2} \mat{1 & i \\ i & 1}
    \mat{e^{it} & 0 \\ 0 & e^{-it}} \mat {1 & -i \\ -i & 1} =
    \frac{1}{2} \mat{e^{it}+e^{-it} & -ie^{it}+ie^{-it} \\ ie^{it}-ie^{-it} &
      e^{it}+e^{-it}} \implies \\ 
    \exp \mat{0 & t \\ -t & 0} = \mat{\cos t & \sin t \\ -\sin t & \cos t}
  \end{gather*}

  Volviendo al sistema que queríamos resolver, es decir, \(x' = \smat{a & b \\
    -b & a}x\), se tiene

  \[A = P \mat{a & b \\ -b & a} P^{-1} \implies e^{At} = P \mat{e^{at}\cos bt &
      e^{at}\sin bt \\ -e^{at} \sin bt & e^{at} \cos bt} P^{-1}\]

  y, así, la solución general del problema es

  \[x(t) = P \mat{e^{at}\cos bt &
      e^{at}\sin bt \\ -e^{at} \sin bt & e^{at} \cos bt} P^{-1} \mat{k_1 \\
      k_2} = P \mat{e^{at}\cos bt & e^{at}\sin bt \\ -e^{at} \sin bt & e^{at}
      \cos bt} \mat {c_1 \\ c_2}\]
  
  En el caso diagonalizable más general, es decir, con autovalores reales y
  complejos, basta combinar todo lo que hemos visto: si \(\lambda_1, \dots,
  \lambda_r\) son los autovalores reales de \(A\) y \(a_1 \pm ib_1, \dots, a_s
  \pm ib_s (b_j>0)\) son sus autovalores complejos (en ambos casos se admiten
  repeticiones), se considera la matriz de paso
  \[P = \mat{v_1 & \cdots & v_r & \Re w_1 & \Im w_1 & \cdots & \Re w_s & \Im w_s}\]
  siendo \(v_j\) y \(w_j\) autovectores asociados a \(\lambda_j\) y \(a_j
  +ib_j\), respectivamente, de forma que se tiene la descomposición

  \[P^{-1}AP = \mat{\lambda_1 & & & & & & & \\
      & \ddots & & & & & & \\
      & & \lambda_r & & & & & \\
      & & & a_1 & b_1 & & & & \\
      & & & -b_1 & a_1 & & & & \\
      & & & & & \ddots & & \\
      & & & & & & a_s & b_s \\
      & & & & & & -b_s & a_s}\]

  y la solución general del sistema es

  \[x(t) = P \mat{e^{\lambda_1 t} & & & & & & & \\
      & \ddots & & & & & & \\
      & & e^{\lambda_r t} & & & & & \\
      & & & e^{a_1 t} \cos b_1 t & e^{a_1 t} \sin b_1 t & & & & \\
      & & & -e^{a_1 t} \sin b_1 t & e^{a_1 t} \cos b_1 t & & & & \\
      & & & & & \ddots & & \\
      & & & & & & e^{a_s t} \cos b_s t & e^{a_s t} \sin b_s t \\
      & & & & & & -e^{a_s t} \sin b_s t & e^{a_s t} \cos b_s t}
    \mat{c_1 \\ \vdots \\ c_r \\ k_1 \\ k_2 \\ \vdots \\ k_{2s-1}\\ k_{2s}}.\]

  
\end{document}
